import streamlit as st
import requests
import json
from typing import Generator
import time

# ==================== PAGE CONFIGURATION ====================
st.set_page_config(
    page_title="Local Ollama Chatbot",
    page_icon="ðŸ’¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for enhanced UI
st.markdown("""
    <style>
    .chat-message {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
    }
    .user-message {
        background-color: #e3f2fd;
        border-left: 4px solid #2196F3;
    }
    .assistant-message {
        background-color: #f5f5f5;
        border-left: 4px solid #4CAF50;
    }
    </style>
""", unsafe_allow_html=True)

# ==================== PAGE HEADER ====================
st.title("ðŸ’¬ Local Ollama Chatbot")
st.markdown("### Powered by TinyDolphin with Streaming Responses")

# ==================== SIDEBAR CONFIGURATION ====================
with st.sidebar:
    st.header("âš™ï¸ Settings")
    
    # Ollama API endpoint configuration
    api_host = st.text_input(
        "Ollama API Host",
        value="http://127.0.0.1:11434",
        help="URL where Ollama is running"
    )
    
    # Model selection
    model_name = st.selectbox(
        "Select Model",
        options=["tinydolphin", "llama2", "mistral", "neural-chat"],
        index=0,
        help="Choose the LLM model to use"
    )
    
    # Temperature setting for response creativity
    temperature = st.slider(
        "Temperature",
        min_value=0.0,
        max_value=1.0,
        value=0.7,
        step=0.1,
        help="Higher = more creative, Lower = more deterministic"
    )
    
    # Top K setting for token selection
    top_k = st.slider(
        "Top K",
        min_value=0,
        max_value=100,
        value=40,
        help="Number of top tokens to consider"
    )
    
    # Top P setting for nucleus sampling
    top_p = st.slider(
        "Top P",
        min_value=0.0,
        max_value=1.0,
        value=0.9,
        step=0.05,
        help="Nucleus sampling parameter"
    )
    
    st.divider()
    
    # Connection status indicator
    if st.button("ðŸ” Check Ollama Connection"):
        try:
            response = requests.get(f"{api_host}/api/tags", timeout=5)
            if response.status_code == 200:
                st.success("âœ… Connected to Ollama!")
                models_data = response.json()
                available_models = [m.get("name", "unknown") for m in models_data.get("models", [])]
                if available_models:
                    st.write("Available models:")
                    for m in available_models:
                        st.write(f"  â€¢ {m}")
            else:
                st.error("âŒ Failed to connect")
        except Exception as e:
            st.error(f"âŒ Connection error: {str(e)}")
    
    st.divider()
    
    # Clear conversation button
    if st.button("ðŸ—‘ï¸ Clear Conversation", use_container_width=True):
        st.session_state.conversation_history = []
        st.session_state.show_confirmation = False
        st.rerun()

# ==================== SESSION STATE INITIALIZATION ====================
if "conversation_history" not in st.session_state:
    st.session_state.conversation_history = []

if "message_count" not in st.session_state:
    st.session_state.message_count = 0

# ==================== OLLAMA API FUNCTIONS ====================
def stream_response(user_message: str, api_host: str, model: str, temp: float, k: int, p: float) -> Generator[str, None, None]:
    """
    Stream response from Ollama API with streaming enabled.
    Yields tokens as they are generated by the model.
    """
    ollama_url = f"{api_host}/api/generate"
    
    payload = {
        "model": model,
        "prompt": user_message,
        "stream": True,
        "temperature": temp,
        "top_k": k,
        "top_p": p
    }
    
    try:
        response = requests.post(ollama_url, json=payload, stream=True, timeout=300)
        response.raise_for_status()
        
        full_response = ""
        for line in response.iter_lines():
            if line:
                try:
                    chunk = json.loads(line)
                    token = chunk.get("response", "")
                    full_response += token
                    yield token
                except json.JSONDecodeError:
                    continue
    
    except requests.exceptions.ConnectionError:
        yield "âŒ Error: Could not connect to Ollama. Please ensure Ollama is running."
    except requests.exceptions.Timeout:
        yield "âŒ Error: Request timed out. The model took too long to respond."
    except Exception as e:
        yield f"âŒ Error: {str(e)}"

def get_full_response(user_message: str, api_host: str, model: str, temp: float, k: int, p: float) -> str:
    """
    Get complete response from Ollama (non-streaming for simpler handling).
    """
    ollama_url = f"{api_host}/api/generate"
    
    payload = {
        "model": model,
        "prompt": user_message,
        "stream": False,
        "temperature": temp,
        "top_k": k,
        "top_p": p
    }
    
    try:
        response = requests.post(ollama_url, json=payload, timeout=300)
        response.raise_for_status()
        response_data = response.json()
        return response_data.get("response", "No response received from model.")
    
    except requests.exceptions.ConnectionError:
        return "âŒ Error: Could not connect to Ollama. Ensure it's running on " + api_host
    except requests.exceptions.Timeout:
        return "âŒ Error: Request timed out. The model took too long to respond."
    except Exception as e:
        return f"âŒ Error: {str(e)}"

# ==================== MAIN CHAT INTERFACE ====================
# Input section at the top
st.subheader("ðŸ’­ Chat")

col1, col2 = st.columns([0.85, 0.15])

with col1:
    user_input = st.text_input(
        label="Your message",
        placeholder="Type your question here...",
        label_visibility="collapsed",
        key="user_input"
    )

with col2:
    send_button = st.button("ðŸ“¤ Send", use_container_width=True)

# Process user input
if send_button and user_input.strip():
    # Add user message to history
    st.session_state.conversation_history.append({
        "role": "user",
        "content": user_input,
        "timestamp": time.strftime("%H:%M:%S")
    })
    
    st.session_state.message_count += 1

if len(st.session_state.conversation_history) > 0 and st.session_state.conversation_history[-1]["role"] == "user":
    # Get the latest user message
    latest_user_message = st.session_state.conversation_history[-1]["content"]
    
    # Display loading indicator
    with st.spinner(f"ðŸ¤– {model_name} is thinking..."):
        # Get response from Ollama
        assistant_response = get_full_response(
            latest_user_message,
            api_host,
            model_name,
            temperature,
            top_k,
            top_p
        )
    
    # Add assistant response to history
    st.session_state.conversation_history.append({
        "role": "assistant",
        "content": assistant_response,
        "timestamp": time.strftime("%H:%M:%S")
    })

# ==================== DISPLAY CONVERSATION HISTORY ====================
st.divider()

if st.session_state.conversation_history:
    st.subheader(f"ðŸ“œ Conversation ({st.session_state.message_count} messages)")
    
    # Display all messages in conversation
    for idx, message in enumerate(st.session_state.conversation_history):
        if message["role"] == "user":
            with st.chat_message("user", avatar="ðŸ‘¤"):
                st.write(message["content"])
                st.caption(f"_Sent at {message.get('timestamp', 'N/A')}_")
        else:
            with st.chat_message("assistant", avatar="ðŸ¤–"):
                st.write(message["content"])
                st.caption(f"_{model_name} at {message.get('timestamp', 'N/A')}_")
else:
    st.info("ðŸ‘‹ Welcome! Start a conversation by typing a message above.")

# ==================== FOOTER ====================
st.divider()
st.markdown("""
    <div style='text-align: center; color: gray; font-size: small;'>
    <p>ðŸš€ Local Ollama Chatbot | Runs entirely on your machine</p>
    <p>Model: <b>{}</b> | Host: <b>{}</b></p>
    </div>
""".format(model_name, api_host), unsafe_allow_html=True)